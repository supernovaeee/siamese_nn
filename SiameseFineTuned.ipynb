{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supernovaeee/siamese_nn/blob/main/SiameseFineTuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5cbf68",
      "metadata": {
        "id": "fc5cbf68"
      },
      "outputs": [],
      "source": [
        "# pip install tensorflow opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e918de",
      "metadata": {
        "id": "85e918de"
      },
      "outputs": [],
      "source": [
        "# pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons[tensorflow]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTfPfHXE0W1h",
        "outputId": "71b3e897-0091-4db4-fa6c-fe94f6b6fdd7"
      },
      "id": "fTfPfHXE0W1h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons[tensorflow]\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons[tensorflow]) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons[tensorflow])\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tensorflow<2.14.0,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.14.0,>=2.11.0->tensorflow-addons[tensorflow]) (3.2.2)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c37fb0",
      "metadata": {
        "id": "c0c37fb0"
      },
      "source": [
        "## 1.2 Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89488b7d",
      "metadata": {
        "id": "89488b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb08a7ca-9483-43d5-9930-e544a63cb65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import glob\n",
        "import string\n",
        "import re\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2424872",
      "metadata": {
        "id": "f2424872"
      },
      "outputs": [],
      "source": [
        "# Import tensorflow dependencies - functional API\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras import layers, models, losses, optimizers\n",
        "from tensorflow.keras.utils import Progbar\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Lambda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c6d0a1",
      "metadata": {
        "id": "f3c6d0a1"
      },
      "outputs": [],
      "source": [
        "# tf.config.list_physical_devices(\"GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644adc0d",
      "metadata": {
        "id": "644adc0d"
      },
      "source": [
        "## 1.3 Create Folder Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa72ef46",
      "metadata": {
        "id": "aa72ef46"
      },
      "outputs": [],
      "source": [
        "# set up paths\n",
        "# for Data datasets (LFW + AFD): drive/MyDrive/data/anchor\n",
        "# for FaceID datasets (LFW): drive/MyDrive/FaceID/data/positive\n",
        "NEG_PATH = os.path.join('drive', 'MyDrive', 'CombinedDataset', 'Negative')\n",
        "POS_PATH = os.path.join('drive', 'MyDrive',  'CombinedDataset', 'Positive')\n",
        "ANC_PATH = os.path.join('drive', 'MyDrive', 'CombinedDataset', 'Anchor')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa87b85",
      "metadata": {
        "id": "4fa87b85"
      },
      "source": [
        "# 2. Collect Positives and Anchors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8cebb1b",
      "metadata": {
        "id": "a8cebb1b"
      },
      "source": [
        "## 2.1 Untar Labelled Faces in the Wild Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3722ffeb",
      "metadata": {
        "id": "3722ffeb"
      },
      "outputs": [],
      "source": [
        "# https://vis-www.cs.umass.edu/lfw/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814dd0ce",
      "metadata": {
        "id": "814dd0ce"
      },
      "outputs": [],
      "source": [
        "# # uncompress tar LFW dataset\n",
        "# !tar -xf lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4fb0980",
      "metadata": {
        "id": "e4fb0980"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Define the letters to check\n",
        "# letters_to_check = string.ascii_uppercase[2::2]  # 'C', 'E', 'G', ...\n",
        "\n",
        "# # Move LFW images to the following repo (data/negative)\n",
        "# for file in os.listdir('FaceID/data/negative'):\n",
        "#     for letter in letters_to_check:\n",
        "#         if file.startswith(letter):\n",
        "#             EX_PATH = os.path.join('FaceID/data/negative', file)\n",
        "#             NEW_PATH = os.path.join(ANC_PATH, file)\n",
        "#             os.renames(EX_PATH, NEW_PATH)\n",
        "#             break  # Break the loop if a match is found\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.listdir(POS_PATH)"
      ],
      "metadata": {
        "id": "i9GaXlSpmSjE"
      },
      "id": "i9GaXlSpmSjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8376ada6",
      "metadata": {
        "id": "8376ada6"
      },
      "outputs": [],
      "source": [
        "# len(os.listdir(POS_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e71a9c3",
      "metadata": {
        "id": "7e71a9c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Move images from anchor to positive if their filenames end with even numbers\n",
        "# for file in os.listdir(POS_PATH):\n",
        "#     if re.match('.*\\_[01][012][0123456789][02468]\\.jpg$', file):\n",
        "#         EX_PATH = os.path.join(POS_PATH, file)\n",
        "#         NEW_PATH = os.path.join(ANC_PATH, file)\n",
        "#         os.renames(EX_PATH, NEW_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5cab56",
      "metadata": {
        "id": "4a5cab56"
      },
      "source": [
        "# 3. Load and Preprocess Images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b3d7dc",
      "metadata": {
        "id": "f4b3d7dc"
      },
      "source": [
        "## 3.1 Get Image Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d492e9",
      "metadata": {
        "id": "a6d492e9"
      },
      "outputs": [],
      "source": [
        "# First, ensure that the image files are correctly stored into anchor, positive, and negative folders\n",
        "# The anchor and positive images should refer to the same person, while negative is for remaining datasets\n",
        "# Split a person's images into anchor and positive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# document_files = os.listdir(POS_PATH)\n",
        "# document_files"
      ],
      "metadata": {
        "id": "yIjmUykuyq0G"
      },
      "id": "yIjmUykuyq0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lByxFKQrxlNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5739eb38-abbd-4267-88aa-ecbae7015676"
      },
      "id": "lByxFKQrxlNK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7143767",
      "metadata": {
        "id": "d7143767"
      },
      "outputs": [],
      "source": [
        "# Get all filenames in the document directory\n",
        "document_files = os.listdir(ANC_PATH)\n",
        "\n",
        "# Extract unique names from the document filenames\n",
        "names = []\n",
        "for filename in document_files:\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) == 3:\n",
        "        # Format: FirstName_LastName_0002.jpg\n",
        "        name = '_'.join(parts[:2]) + '_'\n",
        "    elif len(parts) == 2:\n",
        "        # Format: FirstName_0002.jpg\n",
        "        name = parts[0] + '_'\n",
        "    else:\n",
        "        continue  # Skip files with unexpected format\n",
        "\n",
        "    names.append(name)\n",
        "\n",
        "# Remove duplicates from the names list\n",
        "names = list(set(names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10ff615",
      "metadata": {
        "id": "e10ff615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97c0ba6-ff15-428c-9733-d1c759960e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2935"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "908a2e25",
      "metadata": {
        "id": "908a2e25"
      },
      "outputs": [],
      "source": [
        "# Randomly select names from the available names list\n",
        "# num_names = len(names) - 1\n",
        "selected_names = random.sample(names, 2935)\n",
        "\n",
        "# Create empty anchor and positive datasets\n",
        "anchor_list = []\n",
        "positive_list = []\n",
        "negative = tf.data.Dataset.from_tensor_slices([\"\"])\n",
        "\n",
        "# Load images from anchor and positive folders and pair them\n",
        "for i, name in enumerate(selected_names):\n",
        "    anchor_files = glob.glob(ANC_PATH + f'/{name}*.jpg')\n",
        "    positive_files = glob.glob(POS_PATH + f'/{name}*.jpg')\n",
        "\n",
        "    anchor_num = len(anchor_files)\n",
        "    positive_num = len(positive_files)\n",
        "\n",
        "    if (anchor_num > positive_num):\n",
        "      # cycle through the filenames of the most abundant (if anchor files is more abundant than positive files for this name)\n",
        "      for i in range(anchor_num):\n",
        "        positive_item = positive_files[i % positive_num] # modulo operator makes sure the index used for positive_files never goes out of bounds. It will cycle back to zero (and increment from zero) once the index reaches a multiple of the list's len()\n",
        "        anchor_item = anchor_files[i]\n",
        "\n",
        "        # Append the datasets to anchor and positive\n",
        "        anchor_list.append(anchor_item)\n",
        "        positive_list.append(positive_item)\n",
        "    else:\n",
        "      # cycle through the filenames of the most abundant (else, if positive files is more abundant than anchor files for this name, or they are equal)\n",
        "      for i in range(positive_num):\n",
        "        anchor_item = anchor_files[i % anchor_num] # modulo operator makes sure the index used for anchor_files never goes out of bounds. It will cycle back to zero (and increment from zero) once the index reaches a multiple of the list's len()\n",
        "        positive_item = positive_files[i]\n",
        "        # Append the datasets to anchor and positive\n",
        "        anchor_list.append(anchor_item)\n",
        "        positive_list.append(positive_item)\n",
        "\n",
        "# Turn anchor and positive list into tf.data.Dataset (tensorflow dataset(?))\n",
        "anchor = tf.data.Dataset.from_tensor_slices(anchor_list)\n",
        "positive = tf.data.Dataset.from_tensor_slices(positive_list)\n",
        "\n",
        "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(len(anchor)) # take the length of anchor since length of either one is the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(anchor)"
      ],
      "metadata": {
        "id": "ODQdQ1p7iqbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e021245c-9e5c-4d1e-f862-bf7c8e849ce2"
      },
      "id": "ODQdQ1p7iqbj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2937"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print the elements of the paired datasets -- FOR CHECKING\n",
        "# num_elements_to_show = 30  # Specify the number of elements to show\n",
        "# anchor_samples = anchor.take(num_elements_to_show)\n",
        "# positive_samples = positive.take(num_elements_to_show)\n",
        "\n",
        "# for a, p in tf.data.Dataset.zip((anchor_samples, positive_samples)):\n",
        "#     print(\"Anchor:\", a)\n",
        "#     print(\"Positive:\", p)"
      ],
      "metadata": {
        "id": "SpmB7nVEiQ7-"
      },
      "id": "SpmB7nVEiQ7-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59f2c2d",
      "metadata": {
        "id": "b59f2c2d"
      },
      "outputs": [],
      "source": [
        "# # Print the elements of the paired datasets -- FOR CHECKING\n",
        "# num_elements_to_show = 30  # Specify the number of elements to show\n",
        "# anchor_samples = anchor.take(num_elements_to_show)\n",
        "# negative_samples = negative.take(num_elements_to_show)\n",
        "\n",
        "# for a, p in tf.data.Dataset.zip((anchor_samples, negative_samples)):\n",
        "#     print(\"Anchor:\", a)\n",
        "#     print(\"Negative:\", p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffce634d",
      "metadata": {
        "id": "ffce634d"
      },
      "outputs": [],
      "source": [
        "# # Take a look at the anchor dataset -- FOR CHECKING\n",
        "# dir_test = anchor.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec00508",
      "metadata": {
        "id": "6ec00508"
      },
      "outputs": [],
      "source": [
        "# print(dir_test.next()) -- FOR CHECKING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b3bd14",
      "metadata": {
        "id": "f8b3bd14"
      },
      "source": [
        "## 3.2 Preprocessing - Scale and Resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a44bf1",
      "metadata": {
        "id": "13a44bf1"
      },
      "outputs": [],
      "source": [
        "def preprocess(file_path):\n",
        "\n",
        "    # Read in image from file path\n",
        "    byte_img = tf.io.read_file(file_path)\n",
        "    # Load in the image\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "    # Preprocessing steps - resizing the image to be 100x100x3\n",
        "    img = tf.image.resize(img, (100,100))\n",
        "    # Scale image to be between 0 and 1\n",
        "    img = img / 255.0\n",
        "#\n",
        "    # Return image\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(file_path):\n",
        "\n",
        " # Read in image from file path\n",
        "    byte_img = tf.io.read_file(file_path)\n",
        "    # Load in the image\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "nXZcl8YEkYXB"
      },
      "id": "nXZcl8YEkYXB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b69b99",
      "metadata": {
        "id": "60b69b99"
      },
      "outputs": [],
      "source": [
        "# img = preprocess('drive/MyDrive/FaceID/data/anchor/Mick_Jagger_0002.jpg') -- FOR CHECKING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = process('drive/MyDrive/FaceID/data/anchor/Mick_Jagger_0002.jpg') -- FOR CHECKING\n",
        "# # Display the image using Matplotlib\n",
        "# plt.imshow(x)\n",
        "# plt.axis('off')  # Remove axis ticks\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_TrIWWamkOFJ"
      },
      "id": "_TrIWWamkOFJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert the TensorFlow tensor to a NumPy array for visualization -- FOR CHECKING\n",
        "# img_np = img.numpy()\n",
        "\n",
        "# # Display the image using Matplotlib\n",
        "# plt.imshow(img_np)\n",
        "# plt.axis('off')  # Remove axis ticks\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "9HhqvFIZkFPY"
      },
      "id": "9HhqvFIZkFPY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e986a79e",
      "metadata": {
        "id": "e986a79e"
      },
      "outputs": [],
      "source": [
        "# img.numpy().max() -- FOR CHECKING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6a1585",
      "metadata": {
        "id": "da6a1585"
      },
      "source": [
        "## 3.3 Create Labelled Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff4b2855",
      "metadata": {
        "id": "ff4b2855"
      },
      "outputs": [],
      "source": [
        "# (anchor, positive) => 1,1,1,1,1\n",
        "# (anchor, negative) => 0,0,0,0,0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(negative)"
      ],
      "metadata": {
        "id": "xt_fbq95od2P"
      },
      "id": "xt_fbq95od2P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess(file_path):\n",
        "\n",
        "#     # Read in image from file path\n",
        "#     byte_img = tf.io.read_file(file_path)\n",
        "#     # Load in the image\n",
        "#     img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "#     # Preprocessing steps - resizing the image to be 100x100x3\n",
        "#     img = tf.image.resize(img, (100,100))\n",
        "#     # Scale image to be between 0 and 1\n",
        "#     img = img / 255.0\n",
        "\n",
        "#     # Return image\n",
        "#     return img"
      ],
      "metadata": {
        "id": "GH_77SoPrs2S"
      },
      "id": "GH_77SoPrs2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # def process(file_path):\n",
        "\n",
        "# #     # Read in image from file path\n",
        "# #     byte_img = tf.io.read_file(file_path)\n",
        "# #     # Load in the image\n",
        "# #     img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "# #     return img\n",
        "\n",
        "# def preprocess_twin(input_img, validation_img, label):\n",
        "#     return(preprocess(input_img), preprocess(validation_img), label)"
      ],
      "metadata": {
        "id": "dI3cyHdYrvLg"
      },
      "id": "dI3cyHdYrvLg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load and preprocess the anchor, positive, and negative images\n",
        "# anchor_images = [preprocess(path) for path in anchor]\n",
        "# positive_images = [preprocess(path) for path in positive]\n",
        "# negative_images = [preprocess(path) for path in negative]\n",
        "\n",
        "# # Convert the image lists to tensors\n",
        "# anchor = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
        "# positive = tf.data.Dataset.from_tensor_slices(positive_images)\n",
        "# negative = tf.data.Dataset.from_tensor_slices(negative_images)\n",
        "\n",
        "# # Combine the datasets\n",
        "# positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(positive)))))\n",
        "# negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(negative)))))\n",
        "# data = positives.concatenate(negatives)\n",
        "\n",
        "# # Continue with the rest of the code for creating the Siamese network and training\n",
        "\n"
      ],
      "metadata": {
        "id": "YfgqpB07fxFf"
      },
      "id": "YfgqpB07fxFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d39927",
      "metadata": {
        "id": "56d39927"
      },
      "outputs": [],
      "source": [
        "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(positive)))))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(negative)))))\n",
        "data = positives.concatenate(negatives)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# positives"
      ],
      "metadata": {
        "id": "jAYT2mkpZng9"
      },
      "id": "jAYT2mkpZng9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a652bd",
      "metadata": {
        "id": "09a652bd"
      },
      "outputs": [],
      "source": [
        "# num_elements_to_show = 5  # Specify the number of elements to show\n",
        "# positive_samples = positives.take(num_elements_to_show)\n",
        "\n",
        "# for data_point in positive_samples:\n",
        "#     print(data_point)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NJ356cPITPB4"
      },
      "id": "NJ356cPITPB4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3c715a",
      "metadata": {
        "id": "fc3c715a"
      },
      "outputs": [],
      "source": [
        "# samples = data.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b067f3",
      "metadata": {
        "id": "21b067f3"
      },
      "outputs": [],
      "source": [
        "# examples = samples.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378ee17f",
      "metadata": {
        "id": "378ee17f"
      },
      "source": [
        "## 3.4 Build Train and Test Partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf9c7e2",
      "metadata": {
        "id": "edf9c7e2"
      },
      "outputs": [],
      "source": [
        "def preprocess_twin(input_img, validation_img, label):\n",
        "    return(preprocess(input_img), preprocess(validation_img), label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3423194e",
      "metadata": {
        "id": "3423194e"
      },
      "outputs": [],
      "source": [
        "# res = preprocess_twin(*examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227ff16f",
      "metadata": {
        "id": "227ff16f"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(res[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ec168c",
      "metadata": {
        "id": "53ec168c"
      },
      "outputs": [],
      "source": [
        "# Build dataloader pipeline\n",
        "data = data.map(preprocess_twin)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size = 1024)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "G9K9v1seQ_Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0865a0a0-4be0-4218-b3fe-eecf4381c935"
      },
      "id": "G9K9v1seQ_Ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5874"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ff36af",
      "metadata": {
        "id": "22ff36af"
      },
      "outputs": [],
      "source": [
        "# REPLACED - NOT USED\n",
        "# # Training partition\n",
        "# train_data = data.take(round(len(data)*.7))\n",
        "# # train_data = data.take(4)\n",
        "# print(len(train_data))\n",
        "# train_data = train_data.batch(16)\n",
        "# print(len(train_data))\n",
        "# train_data = train_data.prefetch(8)\n",
        "# print(len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1260a5",
      "metadata": {
        "id": "8e1260a5"
      },
      "outputs": [],
      "source": [
        "# REPLACED - NOT USED\n",
        "# # Testing partition\n",
        "# test_data = data.skip(round(len(data)*.7))\n",
        "# # test_data = data.skip(4)\n",
        "# # test_data = test_data.take(round(len(data)*.3)) # use the dataset that has been skipped the amount of what was taken for train_data\n",
        "# print(len(test_data))\n",
        "# test_data = test_data.batch(16)\n",
        "# print(len(test_data))\n",
        "# test_data = test_data.prefetch(8)\n",
        "# print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAhN6_lfL0K6"
      },
      "id": "HAhN6_lfL0K6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and testing partitions\n",
        "train_size = round(len(data) * 0.7)\n",
        "val_size = round(len(data) * 0.2)\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data.take(train_size)\n",
        "val_data = data.skip(train_size).take(val_size)\n",
        "test_data = data.skip(train_size + val_size)"
      ],
      "metadata": {
        "id": "hICb_pGjT7vq"
      },
      "id": "hICb_pGjT7vq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "1WXKlERKT-2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3858f48c-a0b7-4dfc-eec9-31bd644ffd49"
      },
      "id": "1WXKlERKT-2B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4112\n",
            "1175\n",
            "587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data.element_spec"
      ],
      "metadata": {
        "id": "TX-Eg7MCNjqO"
      },
      "id": "TX-Eg7MCNjqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE) # add , drop_remainder=True after batch_size if want to drop remainder\n",
        "val_data = val_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "lBCEM5JPUF4I"
      },
      "id": "lBCEM5JPUF4I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "id": "9IbjtCpzu4Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dced1b08-ef19-46d8-f05f-a1b486d8e437"
      },
      "id": "9IbjtCpzu4Zd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                                                      # # Convert tf.data.Dataset to NumPy arrays\n",
        "# train_data_np = np.array(list(train_data.as_numpy_iterator()))\n",
        "# val_data_np = np.array(list(val_data.as_numpy_iterator()))"
      ],
      "metadata": {
        "id": "TaBnVjErkqQB"
      },
      "id": "TaBnVjErkqQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_samples = train_data.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "64zTAFBxT24-"
      },
      "id": "64zTAFBxT24-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examples = train_samples.next()"
      ],
      "metadata": {
        "id": "EdH5op65Uibk"
      },
      "id": "EdH5op65Uibk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(examples[0])"
      ],
      "metadata": {
        "id": "z-sVz_p3UkD1"
      },
      "id": "z-sVz_p3UkD1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0299f90",
      "metadata": {
        "id": "d0299f90"
      },
      "outputs": [],
      "source": [
        "# test_data_samples = test_data.as_numpy_iterator()\n",
        "# examples = train_samples.next()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(examples[1])"
      ],
      "metadata": {
        "id": "8NVS40sqV4oi"
      },
      "id": "8NVS40sqV4oi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "980a817f",
      "metadata": {
        "id": "980a817f"
      },
      "source": [
        "# 4. Model Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l1_reg(weight_matrix):\n",
        "  return tf.keras.regularizers.l1(l=0.01)"
      ],
      "metadata": {
        "id": "YshncU51HnH7"
      },
      "id": "YshncU51HnH7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_reg(weight_matrix):\n",
        "  return tf.keras.regularizers.l2(l=0.01)"
      ],
      "metadata": {
        "id": "Uzo5PkhjHomR"
      },
      "id": "Uzo5PkhjHomR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding():\n",
        "    inp = Input(shape=(100,100,3), name='input_image')\n",
        "\n",
        "    # First block\n",
        "    c1 = Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2_reg)(inp)\n",
        "    m1 = MaxPooling2D(pool_size=(2,2))(c1)\n",
        "    b1 = BatchNormalization()(m1)\n",
        "    d1 = Dropout(0.5)(b1)\n",
        "\n",
        "    # Second block\n",
        "    c2 = Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2_reg)(d1)\n",
        "    b2 = BatchNormalization()(c2)\n",
        "    d2 = Dropout(0.5)(b2)\n",
        "\n",
        "    # Final embedding block\n",
        "    f1 = Flatten()(d2)\n",
        "    d1 = Dense(1024, activation='sigmoid', kernel_regularizer=l2_reg)(f1)\n",
        "\n",
        "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "2FoDl_HSLZGN"
      },
      "id": "2FoDl_HSLZGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
      ],
      "metadata": {
        "id": "nXXejc3wU1rZ"
      },
      "id": "nXXejc3wU1rZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def contrastive_loss(left_feature, right_feature, label, margin):\n",
        "#     #   Compute the contrastive loss as in\n",
        "\n",
        "\n",
        "#     # L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
        "\n",
        "#     # **Parameters**\n",
        "#     #  left_feature: First element of the pair ->\n",
        "#     #  right_feature: Second element of the pair\n",
        "#     #  label: Label of the pair (0 or 1)\n",
        "#     #  margin: Contrastive margin\n",
        "\n",
        "#     # **Returns**\n",
        "#     #  Return the loss operation\n",
        "#     # Calculate loss for similar pairs (y_true == 0)\n",
        "\n",
        "#     positive_loss = (1 - y_true) * tf.square(distances)\n",
        "\n",
        "#     # Calculate loss for dissimilar pairs (y_true == 1)\n",
        "#     negative_loss = y_true * tf.square(tf.maximum(margin - distances, 0))\n",
        "\n",
        "#     # Combine positive and negative losses\n",
        "#     loss = positive_loss + negative_loss\n",
        "\n",
        "#     # Calculate the mean loss over the batch\n",
        "#     mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "#     return mean_loss\n"
      ],
      "metadata": {
        "id": "83uuD37SFPgW"
      },
      "id": "83uuD37SFPgW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_euclidean_distance(x, y):\n",
        "#     \"\"\"\n",
        "#     Computes the euclidean distance between two tensorflow variables\n",
        "#     \"\"\"\n",
        "\n",
        "#     d = tf.reduce_sum(tf.square(tf.sub(x, y)),1)\n",
        "#     return d"
      ],
      "metadata": {
        "id": "eYo4wnGjqjOB"
      },
      "id": "eYo4wnGjqjOB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_contrastive_loss(left_feature, right_feature, label, margin):\n",
        "\n",
        "#     \"\"\"\n",
        "#     Compute the contrastive loss as in\n",
        "\n",
        "\n",
        "#     L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
        "\n",
        "#     **Parameters**\n",
        "#      left_feature: First element of the pair\n",
        "#      right_feature: Second element of the pair\n",
        "#      label: Label of the pair (0 or 1)\n",
        "#      margin: Contrastive margin\n",
        "\n",
        "#     **Returns**\n",
        "#      Return the loss operation\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     label = tf.to_float(label)\n",
        "#     one = tf.constant(1.0)\n",
        "\n",
        "#     d = compute_euclidean_distance(left_feature, right_feature)\n",
        "#     d_sqrt = tf.sqrt(compute_euclidean_distance(left_feature, right_feature))\n",
        "#     first_part = tf.mul(one-label, d)# (Y-1)*(d)\n",
        "\n",
        "#     max_part = tf.square(tf.maximum(margin-d_sqrt, 0))\n",
        "#     second_part = tf.mul(label, max_part)  # (Y) * max(margin - d, 0)\n",
        "\n",
        "#     loss = 0.5 * tf.reduce_mean(first_part + second_part)\n",
        "\n",
        "#     return loss"
      ],
      "metadata": {
        "id": "Dd-6KKNvqk8q"
      },
      "id": "Dd-6KKNvqk8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def triplet_loss(y_true, y_pred, alpha=0.2):\n",
        "#     anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "\n",
        "#     # Compute squared distances\n",
        "#     pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "#     neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "\n",
        "#     # Compute triplet loss\n",
        "#     basic_loss = pos_dist - neg_dist + alpha\n",
        "#     loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
        "\n",
        "#     return loss"
      ],
      "metadata": {
        "id": "aynWnwvNilzL"
      },
      "id": "aynWnwvNilzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding()"
      ],
      "metadata": {
        "id": "QmkHUPWbC7lM"
      },
      "id": "QmkHUPWbC7lM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.summary()"
      ],
      "metadata": {
        "id": "u9TAjaUvQUVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f58efa9-b383-4bfb-d559-41ea0192778d"
      },
      "id": "u9TAjaUvQUVz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 98, 98, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 49, 49, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 49, 49, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 49, 49, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 47, 47, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 47, 47, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 47, 47, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 141376)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              144770048 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 144,789,824\n",
            "Trainable params: 144,789,632\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Siamese L1 Distance class -- possible to change to equillibrium distance\n",
        "class L1Dist(Layer):\n",
        "\n",
        "    # Init method - inheritance\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    # Magic happens here - similarity calculation\n",
        "    def call(self, input_embedding, validation_embedding):\n",
        "        return tf.math.abs(input_embedding - validation_embedding)"
      ],
      "metadata": {
        "id": "WV8NF2Gmbfi2"
      },
      "id": "WV8NF2Gmbfi2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = L1Dist()"
      ],
      "metadata": {
        "id": "wR9wEtIDbn3A"
      },
      "id": "wR9wEtIDbn3A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_siamese_model():\n",
        "\n",
        "  # Anchor image input in the network\n",
        "  input_image = Input(name='input_img', shape=(100,100,3))\n",
        "\n",
        "  # Validation image in the network\n",
        "  validation_image = Input(name='validation_img', shape=(100,100,3))\n",
        "\n",
        "  # Combine siamese distance components\n",
        "  siamese_layer = L1Dist()\n",
        "  siamese_layer._name = 'distance'\n",
        "  distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
        "\n",
        "  # Classification layer\n",
        "  classifier = Dense(1, activation='sigmoid')(distances)\n",
        "\n",
        "  return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
      ],
      "metadata": {
        "id": "z2yZhMXKCSbP"
      },
      "id": "z2yZhMXKCSbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = make_siamese_model()\n",
        "siamese_model.compile(loss = contrastive_loss, optimizer=tf.keras.optimizers.Adam(1e-4))"
      ],
      "metadata": {
        "id": "y078yhNoCwzG"
      },
      "id": "y078yhNoCwzG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "joySCiMjQX6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f524cd-8cbd-4698-ca3d-b418d9299d5e"
      },
      "id": "joySCiMjQX6c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SiameseNetwork\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " embedding (Functional)         (None, 1024)         144789824   ['input_img[0][0]',              \n",
            "                                                                  'validation_img[0][0]']         \n",
            "                                                                                                  \n",
            " distance (L1Dist)              (None, 1024)         0           ['embedding[0][0]',              \n",
            "                                                                  'embedding[1][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            1025        ['distance[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 144,790,849\n",
            "Trainable params: 144,790,657\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load pre-trained MobileNetV2 model without top (classification) layers\n",
        "# base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(100, 100, 3))\n",
        "\n",
        "# # Freeze the pre-trained layers\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # Siamese network architecture on top of the pre-trained model\n",
        "# input_a = Input(shape=(100, 100, 3))\n",
        "# input_b = Input(shape=(100, 100, 3))\n",
        "\n",
        "# embedding_a = base_model(input_a)\n",
        "# embedding_b = base_model(input_b)"
      ],
      "metadata": {
        "id": "ydb2Jw4tCB8u"
      },
      "id": "ydb2Jw4tCB8u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create the Siamese network model\n",
        "# siamese_model = make_siamese_model()\n",
        "\n",
        "# # Compile and train the Siamese network\n",
        "# siamese_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=contrastive_loss)"
      ],
      "metadata": {
        "id": "0ttZQhlTIEm8"
      },
      "id": "0ttZQhlTIEm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5fad5458",
      "metadata": {
        "id": "5fad5458"
      },
      "source": [
        "# 5. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68c6beb",
      "metadata": {
        "id": "b68c6beb"
      },
      "source": [
        "## 5.1 Setup Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1dfec63",
      "metadata": {
        "id": "a1dfec63"
      },
      "outputs": [],
      "source": [
        "binary_cross_loss = tf.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf43225",
      "metadata": {
        "id": "9bf43225"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9828022e",
      "metadata": {
        "id": "9828022e"
      },
      "source": [
        "## 5.2 Build Train Step Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(batch):\n",
        "    # Record all of our operations\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get anchor and positive/negative image\n",
        "        X = batch[:2]\n",
        "        # Get label\n",
        "        y = batch[2]\n",
        "\n",
        "        # Forward pass\n",
        "        yhat = siamese_model(X, training=True)\n",
        "        # Calculate loss\n",
        "        loss = contrastive_loss(y, yhat)\n",
        "\n",
        "        # # Forward pass\n",
        "        # anchor_embedding = embedding(X[0])  # Embedding of anchor image\n",
        "        # positive_embedding = embedding(X[1])  # Embedding of positive image\n",
        "\n",
        "        # # Calculate Euclidean distance (L2 norm) between embeddings\n",
        "        # D = tf.norm(anchor_embedding - positive_embedding, axis=-1)  # Euclidean distance\n",
        "\n",
        "        # # Calculate loss\n",
        "        # loss = contrastive_loss(y, D)\n",
        "\n",
        "    # Calculate gradients\n",
        "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "    # Calculate updated weights and apply to siamese model\n",
        "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
        "\n",
        "    # Return loss\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Mwsr8VzyRauD"
      },
      "id": "Mwsr8VzyRauD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4889a8ab",
      "metadata": {
        "id": "4889a8ab"
      },
      "source": [
        "## 5.3 Build Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training function with validation\n",
        "# def train_with_validation(train_data, val_data, EPOCHS):\n",
        "#     train_losses = []  # To store training losses\n",
        "#     val_losses = []    # To store validation losses\n",
        "\n",
        "#     for epoch in range(1, EPOCHS+1):\n",
        "#         print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
        "#         progbar = tf.keras.utils.Progbar(len(train_data))\n",
        "#         batches = 0  # To track the number of batches processed during training\n",
        "\n",
        "#         # Training loop\n",
        "#         for idx, batch in enumerate(train_data):\n",
        "#             loss = train_step(batch).numpy()\n",
        "#             train_losses.append(loss)\n",
        "#             progbar.update(idx + 1)\n",
        "#             batches += 1\n",
        "\n",
        "#         # Validation loop\n",
        "#         val_loss = 0.0\n",
        "#         for idx, batch in enumerate(val_data):\n",
        "#             val_loss += train_step(batch).numpy()\n",
        "#         val_loss /= len(val_data)\n",
        "#         val_losses.extend([val_loss] * batches)  # Extend the list to match train_losses\n",
        "\n",
        "#         # Print training and validation losses\n",
        "#         print(f'\\nTraining Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}')\n",
        "\n",
        "#     # Plot the training and validation losses\n",
        "#     plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss')\n",
        "#     plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
        "#     plt.xlabel('Training Steps')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "id": "ciop--T1UOMh"
      },
      "id": "ciop--T1UOMh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_validation(train_data, val_data, EPOCHS):\n",
        "    train_losses = []  # To store training losses\n",
        "    val_losses = []    # To store validation losses\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
        "        progbar = tf.keras.utils.Progbar(len(train_data))\n",
        "        batches = 0  # To track the number of batches processed during training\n",
        "\n",
        "        # Training loop\n",
        "        for idx, batch in enumerate(train_data):\n",
        "            loss = train_step(batch).numpy()\n",
        "            train_losses.append(loss)\n",
        "            progbar.update(idx + 1)\n",
        "            batches += 1\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss_batchwise = []\n",
        "        for idx, batch in enumerate(val_data):\n",
        "            val_loss_batchwise.append(train_step(batch).numpy())\n",
        "        val_loss = np.mean(val_loss_batchwise)\n",
        "        val_losses.extend([val_loss] * batches)  # Extend the list to match train_losses\n",
        "\n",
        "        # Print training and validation losses\n",
        "        print(f'\\nTraining Loss: {train_losses[-1]}, Validation Loss: {val_loss}')\n",
        "\n",
        "    # Plot the training and validation losses\n",
        "    plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jHgSQ-We5c5L"
      },
      "id": "jHgSQ-We5c5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46d985b5",
      "metadata": {
        "id": "46d985b5"
      },
      "source": [
        "## 5.4 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bcd037f",
      "metadata": {
        "id": "7bcd037f"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cdaaf5",
      "metadata": {
        "id": "61cdaaf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7fee94-64f4-4502-a136-9fe444556815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "257/257 [==============================] - 468s 1s/step\n",
            "\n",
            "Training Loss: 0.16150307655334473, Validation Loss: 0.07736042141914368\n",
            "\n",
            "Epoch 2/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.1604180932044983, Validation Loss: 0.07295187562704086\n",
            "\n",
            "Epoch 3/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.20453064143657684, Validation Loss: 0.0780431479215622\n",
            "\n",
            "Epoch 4/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.07063174247741699, Validation Loss: 0.06898830831050873\n",
            "\n",
            "Epoch 5/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.07241958379745483, Validation Loss: 0.06774327158927917\n",
            "\n",
            "Epoch 6/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.12934933602809906, Validation Loss: 0.06872374564409256\n",
            "\n",
            "Epoch 7/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.20970626175403595, Validation Loss: 0.07593211531639099\n",
            "\n",
            "Epoch 8/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.06829541176557541, Validation Loss: 0.0748375803232193\n",
            "\n",
            "Epoch 9/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.07441934198141098, Validation Loss: 0.07636623084545135\n",
            "\n",
            "Epoch 10/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.06843581795692444, Validation Loss: 0.07668355107307434\n",
            "\n",
            "Epoch 11/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.1666671633720398, Validation Loss: 0.07656723260879517\n",
            "\n",
            "Epoch 12/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.031001422554254532, Validation Loss: 0.08050765097141266\n",
            "\n",
            "Epoch 13/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.13367091119289398, Validation Loss: 0.08993934094905853\n",
            "\n",
            "Epoch 14/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.1332399845123291, Validation Loss: 0.0787060409784317\n",
            "\n",
            "Epoch 15/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.07357817888259888, Validation Loss: 0.08275908976793289\n",
            "\n",
            "Epoch 16/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.08581767976284027, Validation Loss: 0.08291804790496826\n",
            "\n",
            "Epoch 17/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.15065570175647736, Validation Loss: 0.07799051702022552\n",
            "\n",
            "Epoch 18/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.2065589427947998, Validation Loss: 0.07055215537548065\n",
            "\n",
            "Epoch 19/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.1347624808549881, Validation Loss: 0.08664659410715103\n",
            "\n",
            "Epoch 20/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.1585744321346283, Validation Loss: 0.07004834711551666\n",
            "\n",
            "Epoch 21/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.12016122043132782, Validation Loss: 0.07558239251375198\n",
            "\n",
            "Epoch 22/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.07950610667467117, Validation Loss: 0.07242763787508011\n",
            "\n",
            "Epoch 23/50\n",
            "257/257 [==============================] - 15s 56ms/step\n",
            "\n",
            "Training Loss: 0.24307392537593842, Validation Loss: 0.0755278542637825\n",
            "\n",
            "Epoch 24/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.03438112884759903, Validation Loss: 0.07591597735881805\n",
            "\n",
            "Epoch 25/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.08891352266073227, Validation Loss: 0.07539508491754532\n",
            "\n",
            "Epoch 26/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.08496176451444626, Validation Loss: 0.07836027443408966\n",
            "\n",
            "Epoch 27/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.0740048959851265, Validation Loss: 0.07092119753360748\n",
            "\n",
            "Epoch 28/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.10571935772895813, Validation Loss: 0.08276913315057755\n",
            "\n",
            "Epoch 29/50\n",
            "257/257 [==============================] - 14s 56ms/step\n",
            "\n",
            "Training Loss: 0.16527606546878815, Validation Loss: 0.07840213179588318\n",
            "\n",
            "Epoch 30/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.13752591609954834, Validation Loss: 0.07593677192926407\n",
            "\n",
            "Epoch 31/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.19034576416015625, Validation Loss: 0.07299616187810898\n",
            "\n",
            "Epoch 32/50\n",
            "257/257 [==============================] - 15s 57ms/step\n",
            "\n",
            "Training Loss: 0.17538891732692719, Validation Loss: 0.07207266986370087\n",
            "\n",
            "Epoch 33/50\n",
            "103/257 [===========>..................] - ETA: 8s"
          ]
        }
      ],
      "source": [
        "train_with_validation(train_data, val_data, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # # Call the training function with early stopping\n",
        "# train_with_early_stopping(train_data, val_data, EPOCHS=5, patience=2)"
      ],
      "metadata": {
        "id": "w09Jy2yJeCtw"
      },
      "id": "w09Jy2yJeCtw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d5c7ec3",
      "metadata": {
        "id": "8d5c7ec3"
      },
      "source": [
        "# 6. Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3b1585",
      "metadata": {
        "id": "9a3b1585"
      },
      "source": [
        "## 6.1 Import Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494558de",
      "metadata": {
        "id": "494558de"
      },
      "outputs": [],
      "source": [
        "# import metric calculations\n",
        "from tensorflow.keras.metrics import Precision, Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f76b1ab3",
      "metadata": {
        "id": "f76b1ab3"
      },
      "source": [
        "## 6.2 Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827225ab",
      "metadata": {
        "id": "827225ab"
      },
      "outputs": [],
      "source": [
        "# get entire test data\n",
        "\n",
        "# Initialize empty lists\n",
        "test_input_list = []\n",
        "test_val_list = []\n",
        "y_true_list = []\n",
        "# Iterate over the test_data and collect the data\n",
        "\n",
        "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
        "    test_input_list.append(test_input)\n",
        "    test_val_list.append(test_val)\n",
        "    y_true_list.append(y_true)\n",
        "\n",
        "# Concatenate the collected data into numpy arrays\n",
        "test_input = np.concatenate(test_input_list)\n",
        "test_val = np.concatenate(test_val_list)\n",
        "y_true = np.concatenate(y_true_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f15383",
      "metadata": {
        "id": "58f15383"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_hat = siamese_model.predict([test_input, test_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb13830f",
      "metadata": {
        "scrolled": true,
        "id": "fb13830f"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to binary values\n",
        "y_pred = np.where(y_hat > 0.5, 1, 0)\n",
        "y = np.column_stack((y_pred, y_true))\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "# print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88f6c8d",
      "metadata": {
        "id": "e88f6c8d"
      },
      "source": [
        "## 6.3 Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493918ed",
      "metadata": {
        "id": "493918ed"
      },
      "outputs": [],
      "source": [
        "# Recall (Sensitivity) = TP / TP + FN\n",
        "# Percentage of positive samples correctly predicted (over what is really positive)\n",
        "\n",
        "# Creating a metric object\n",
        "m = Recall()\n",
        "\n",
        "# Calculating the recall value\n",
        "m.update_state(y_true, y_pred)\n",
        "\n",
        "# Return Recall result\n",
        "m.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d0f886",
      "metadata": {
        "id": "96d0f886"
      },
      "outputs": [],
      "source": [
        "# Precision = TP / TP + FP\n",
        "# Percentage of samples that were properly labeled positives (over positive labelled samples)\n",
        "# Creating a metric object\n",
        "m = Precision()\n",
        "\n",
        "# Calculating the precision value\n",
        "m.update_state(y_true, y_pred)\n",
        "\n",
        "# Return Precision result\n",
        "m.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c1f9a9",
      "metadata": {
        "id": "73c1f9a9"
      },
      "outputs": [],
      "source": [
        "# Accuracy = TP + TN / All\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Compute the accuracy score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acea6d3",
      "metadata": {
        "scrolled": true,
        "id": "2acea6d3"
      },
      "outputs": [],
      "source": [
        "# # Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"True Negatives: \", tn, \"False Positives: \", fp, \"\\nFalse Negatives: \", fn, \"True Positives: \", tp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on validation data"
      ],
      "metadata": {
        "id": "iuEOqfK1x5NO"
      },
      "id": "iuEOqfK1x5NO"
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_list = []\n",
        "val_val_list = []\n",
        "y_val_true_list = []\n"
      ],
      "metadata": {
        "id": "SPLzKeUfx7Ax"
      },
      "id": "SPLzKeUfx7Ax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val_input, val_val, y_val_true in val_data.as_numpy_iterator():\n",
        "    val_input_list.append(val_input)\n",
        "    val_val_list.append(val_val)\n",
        "    y_val_true_list.append(y_val_true)\n"
      ],
      "metadata": {
        "id": "kfxpiXmZx8m_"
      },
      "id": "kfxpiXmZx8m_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input = np.concatenate(val_input_list)\n",
        "val_val = np.concatenate(val_val_list)\n",
        "y_val_true = np.concatenate(y_val_true_list)\n"
      ],
      "metadata": {
        "id": "ARM2KEUxx-BA"
      },
      "id": "ARM2KEUxx-BA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_hat = siamese_model.predict([val_input, val_val])\n"
      ],
      "metadata": {
        "id": "0zQsERNmyAN-"
      },
      "id": "0zQsERNmyAN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = np.where(y_val_hat > 0.5, 1, 0)\n"
      ],
      "metadata": {
        "id": "F5PTcWcxyIZd"
      },
      "id": "F5PTcWcxyIZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# ... [earlier code for preparing val_data and making predictions]\n",
        "\n",
        "# Convert predictions to binary values (if you haven't already)\n",
        "y_val_pred = np.where(y_val_hat > 0.5, 1, 0)\n",
        "\n",
        "# Recall\n",
        "m_recall = Recall()\n",
        "m_recall.update_state(y_val_true, y_val_pred)\n",
        "recall = m_recall.result().numpy()\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Precision\n",
        "m_precision = Precision()\n",
        "m_precision.update_state(y_val_true, y_val_pred)\n",
        "precision = m_precision.result().numpy()\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_val_true, y_val_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_val_true, y_val_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)\n",
        "print(\"True Positives: \", tp)\n"
      ],
      "metadata": {
        "id": "Gpoqi6ZSyh1B"
      },
      "id": "Gpoqi6ZSyh1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "85cbbde8",
      "metadata": {
        "id": "85cbbde8"
      },
      "source": [
        "## 6.4 Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad7a836",
      "metadata": {
        "id": "4ad7a836"
      },
      "outputs": [],
      "source": [
        "# Set plot size\n",
        "plt.figure(figsize=(18,8))\n",
        "\n",
        "# Set index\n",
        "# index = 15\n",
        "index = random.randint(0,400)\n",
        "print(\"Index: \", index)\n",
        "\n",
        "# Set first subplot\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_input[index])\n",
        "\n",
        "# Set second subplot\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(test_val[index])\n",
        "\n",
        "# Renders cleanly\n",
        "plt.show()\n",
        "\n",
        "if (y_pred[index] == 1):\n",
        "    print(\"Same Person\")\n",
        "else:\n",
        "    print(\"Different Persons\")\n",
        "\n",
        "print(y_hat[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8481bbf7",
      "metadata": {
        "id": "8481bbf7"
      },
      "source": [
        "# 7. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e96680",
      "metadata": {
        "id": "48e96680"
      },
      "outputs": [],
      "source": [
        "# # Save weights\n",
        "siamese_model.save('drive/MyDrive/h5/siamesemodelFT_contrastive_full_L2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 8. Real Time Test <h1>\n"
      ],
      "metadata": {
        "id": "pUOOUeX54Yxz"
      },
      "id": "pUOOUeX54Yxz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 8.1 Verification Function <h2>"
      ],
      "metadata": {
        "id": "qMjwbPNo4q2s"
      },
      "id": "qMjwbPNo4q2s"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def preprocess(file_path):\n",
        "\n",
        "    # Read in image from file path\n",
        "    byte_img = tf.io.read_file(file_path)\n",
        "    # Load in the image\n",
        "    img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "    # Preprocessing steps - resizing the image to be 100x100x3\n",
        "    img = tf.image.resize(img, (100,100))\n",
        "    # Scale image to be between 0 and 1\n",
        "    img = img / 255.0\n",
        "#\n",
        "    # Return image\n",
        "    return img\n",
        "\n",
        "def plot_results_distribution(results):\n",
        "    # Flatten the results array\n",
        "    flat_results = np.array(results).flatten()\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(flat_results, bins=20, color='blue', alpha=0.7)\n",
        "    plt.xlabel('Prediction Scores')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Prediction Scores')\n",
        "    plt.show()\n",
        "\n",
        "def verify(model, detection_threshold, verification_threshold):\n",
        "    input_images_folder = os.path.join('drive', 'MyDrive', 'FaceID', 'application_data', 'input_images')\n",
        "    verification_images_folder = os.path.join('drive', 'MyDrive', 'FaceID', 'application_data', 'verification_images')\n",
        "\n",
        "    input_images = [os.path.join(input_images_folder, image) for image in os.listdir(input_images_folder)]\n",
        "    verification_images = [os.path.join(verification_images_folder, image) for image in os.listdir(verification_images_folder)]\n",
        "\n",
        "    results = []\n",
        "    for input_img_path in input_images:\n",
        "        input_img = preprocess(input_img_path)\n",
        "        for verification_img_path in verification_images:\n",
        "            validation_img = preprocess(verification_img_path)\n",
        "\n",
        "\n",
        "            result = model.predict([np.expand_dims(input_img, axis=0), np.expand_dims(validation_img, axis=0)])\n",
        "            # print(result)\n",
        "            results.append(result)\n",
        "\n",
        "            input_img_np = input_img.numpy()\n",
        "            validation_img_np = validation_img.numpy()\n",
        "\n",
        "            # # Set first subplot\n",
        "            # plt.subplot(1,2,1)\n",
        "            # plt.imshow(input_img_np)\n",
        "\n",
        "            # # Set second subplot\n",
        "            # plt.subplot(1,2,2)\n",
        "            # plt.imshow(validation_img_np)\n",
        "\n",
        "            # # Renders cleanly\n",
        "            # plt.show()\n",
        "\n",
        "\n",
        "    max = np.max(np.array(results))\n",
        "    print(\"Max. prediction = \", max)\n",
        "    min = np.min(np.array(results))\n",
        "    print(\"Min. prediction = \", min)\n",
        "\n",
        "    # Calculate the median\n",
        "    median = np.median(np.array(results))\n",
        "\n",
        "    # Calculate the mean\n",
        "    mean = np.mean(np.array(results))\n",
        "\n",
        "    print(\"Median:\", median)\n",
        "    print(\"Mean:\", mean)\n",
        "\n",
        "    detection = np.sum(np.array(results) > detection_threshold)\n",
        "    print(\"No. of results above threshold: \", detection)\n",
        "\n",
        "    undetection = np.sum(np.array(results) <= detection_threshold)\n",
        "    print(\"No. of results below threshold: \", undetection)\n",
        "\n",
        "\n",
        "    total_verification_pairs = len(input_images) * len(verification_images)\n",
        "    print(\"No. of total verification pairs: \", total_verification_pairs)\n",
        "    verification = detection / total_verification_pairs\n",
        "    print(\"Verification\", verification)\n",
        "    verified = verification > verification_threshold\n",
        "    return results, verified\n",
        "\n",
        "# Example usage\n",
        "detection_threshold = 0.5\n",
        "verification_threshold = 0.8\n",
        "model = tf.keras.models.load_model('drive/MyDrive/h5/siamesemodelFT_contrastive_full_L2.h5', custom_objects={'contrastive_loss': contrastive_loss, 'L1Dist':L1Dist, 'l2_reg': l2_reg})\n",
        "\n",
        "results, verified = verify(model, detection_threshold, verification_threshold)\n",
        "print(\"Results:\", results)\n",
        "print(\"Verified:\", verified)\n",
        "\n",
        "# Plot the distribution of prediction scores\n",
        "plot_results_distribution(results)\n"
      ],
      "metadata": {
        "id": "SqrMvUTX-A3s"
      },
      "id": "SqrMvUTX-A3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exgZLaz-DuNx"
      },
      "id": "exgZLaz-DuNx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}